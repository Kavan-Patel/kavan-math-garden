{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from numpy.random import seed\n",
    "seed(888)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(404)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from time import strftime\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "784\n"
     ]
    }
   ],
   "source": [
    "FEATURES_TRAIN_PATH=\"MNIST/digit_xtrain.csv\"\n",
    "FEATURES_TEST_PATH=\"MNIST/digit_xtest.csv\"\n",
    "TARGET_TRAIN_PATH=\"MNIST/digit_ytrain.csv\"\n",
    "TARGET_TEST_PATH=\"MNIST/digit_ytest.csv\"\n",
    "\n",
    "LOGGING_PATH='tensorboard_mnist_digit_log/'\n",
    "\n",
    "NR_CLASSES=10\n",
    "VALIDATION_SIZE=10000\n",
    "IMAGE_WIDTH=28\n",
    "IMAGE_HEIGHT=28\n",
    "CHANNELS=1\n",
    "TOTAL_INPUT=IMAGE_HEIGHT*IMAGE_WIDTH*CHANNELS\n",
    "print(TOTAL_INPUT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the Data and Print Shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000,)\n",
      "(10000,)\n",
      "CPU times: user 567 ms, sys: 12.8 ms, total: 580 ms\n",
      "Wall time: 592 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "target_train_all= np.loadtxt(TARGET_TRAIN_PATH, delimiter=',', dtype=int)\n",
    "print(target_train_all.shape)\n",
    "target_test= np.loadtxt(TARGET_TEST_PATH, delimiter=',', dtype=int)\n",
    "print(target_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 13s, sys: 5.33 s, total: 1min 18s\n",
      "Wall time: 1min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "features_train_all= np.loadtxt(FEATURES_TRAIN_PATH, delimiter=',', dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.4 s, sys: 364 ms, total: 11.8 s\n",
      "Wall time: 11.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "features_test= np.loadtxt(FEATURES_TEST_PATH, delimiter=',', dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explorar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First three:  [1, 2, 3]\n",
      "Other then First three:  [4, 5, 6, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "t=[1,2,3,4,5,6,7,8,9]\n",
    "print('First three: ',t[:3])\n",
    "print('Other then First three: ',t[3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features_train_all shape :  (60000, 784)\n",
      "target_train_all shape :  (60000,)\n",
      "features_test shape :  (10000, 784)\n",
      "target_test shape :  (10000,)\n"
     ]
    }
   ],
   "source": [
    "print(\"features_train_all shape : \",features_train_all.shape)\n",
    "print(\"target_train_all shape : \",target_train_all.shape)\n",
    "print(\"features_test shape : \",features_test.shape)\n",
    "print(\"target_test shape : \",target_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First five values of train lable :  [5 0 4 1 9]\n"
     ]
    }
   ],
   "source": [
    "print(\"First five values of train lable : \",target_train_all[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## re-scaling data between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it is for features\n",
    "features_train_all,features_test= features_train_all/255.0 , features_test/255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## convert target values into one hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
    " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
    " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
    " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
    " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "target_train_all= np.eye(NR_CLASSES)[target_train_all]\n",
    "print(target_train_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "target_test = np.eye(NR_CLASSES)[target_test]\n",
    "print(target_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(np.eye(NR_CLASSES)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create validation dataset from training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation data size:  (10000, 784)\n",
      "validation target data side:  (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "features_val = features_train_all[:VALIDATION_SIZE]\n",
    "target_val = target_train_all[:VALIDATION_SIZE]\n",
    "print(\"validation data size: \",features_val.shape)\n",
    "print(\"validation target data side: \",target_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training features data size:  (50000, 784)\n",
      "Training target data size:  (50000, 10)\n"
     ]
    }
   ],
   "source": [
    "features_train = features_train_all[VALIDATION_SIZE:]\n",
    "target_train = target_train_all[VALIDATION_SIZE:]\n",
    "print(\"Training features data size: \",features_train.shape)\n",
    "print(\"Training target data size: \",target_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Tensorflow Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the placeholder to feed the data afterward\n",
    "FEATURES = tf.placeholder(tf.float32, shape=[None, TOTAL_INPUT], name='FEATURES')\n",
    "TARGET = tf.placeholder(tf.float32, shape=[None, NR_CLASSES], name='TARGET')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NeuralNetwrok Architecture\n",
    "###### Hyper Parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.001"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# It will run in 5 throught the data\n",
    "nr_epoch= 50\n",
    "# 0.001\n",
    "learning_rate= 1e-3 \n",
    "# number of neurons in first hidden layer\n",
    "n_hidden1=512\n",
    "# number of neurons in second hidden layer\n",
    "n_hidden2=64\n",
    "1e-3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_layer(input, weight_dim, bias_dim, name):\n",
    "    with tf.name_scope(name):\n",
    "        \n",
    "        initial_w = tf.truncated_normal(shape=weight_dim, stddev=0.1, seed=42)\n",
    "        w = tf.Variable(initial_value= initial_w, name='W')\n",
    "\n",
    "        initial_b = tf.constant(value=0.0, shape=bias_dim)\n",
    "        b= tf.Variable(initial_value= initial_b, name='B')\n",
    "\n",
    "        layer_in= tf.matmul(input,w) + b\n",
    "        \n",
    "        tf.summary.histogram('weight',w)\n",
    "        tf.summary.histogram('bias',b)\n",
    "        \n",
    "        if name==\"out\":\n",
    "            layer_out= tf.nn.softmax(layer_in)\n",
    "        else:\n",
    "            layer_out= tf.nn.relu(layer_in)\n",
    "        return layer_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 1 Without dropout\n",
    "# layer_1 = setup_layer(FEATURES, weight_dim=[TOTAL_INPUT,n_hidden1],\n",
    "#                       bias_dim=[n_hidden1], name='layer_1')\n",
    "# layer_2 = setup_layer(layer_1, weight_dim=[n_hidden1,n_hidden2],\n",
    "#                       bias_dim=[n_hidden2], name='layer_2')\n",
    "# output = setup_layer(layer_2, weight_dim=[n_hidden2,NR_CLASSES],\n",
    "#                       bias_dim=[NR_CLASSES], name='out')\n",
    "# model_name= f'{n_hidden1}-{n_hidden2} LR{learning_rate} E{epoch}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-20-6b37683c814f>:4: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "layer_1 = setup_layer(FEATURES, weight_dim=[TOTAL_INPUT,n_hidden1],\n",
    "                      bias_dim=[n_hidden1], name='layer_1')\n",
    "\n",
    "layer_drop = tf.nn.dropout(layer_1, keep_prob=0.8, name='dropout_layer')\n",
    "\n",
    "layer_2 = setup_layer(layer_drop, weight_dim=[n_hidden1,n_hidden2],\n",
    "                      bias_dim=[n_hidden2], name='layer_2')\n",
    "\n",
    "output = setup_layer(layer_2, weight_dim=[n_hidden2,NR_CLASSES],\n",
    "                      bias_dim=[NR_CLASSES], name='out')\n",
    "\n",
    "model_name= f'{n_hidden1}-DO-{n_hidden2} LR{learning_rate} E{nr_epoch}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorboard Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory is successfully created!\n"
     ]
    }
   ],
   "source": [
    "# Folder for tensorboard\n",
    "folder_name=f'{model_name} at {strftime(\"%I:%M\")}'\n",
    "directory= os.path.join(LOGGING_PATH, folder_name)\n",
    "\n",
    "try:\n",
    "    os.makedirs(directory)\n",
    "except OSError as exception:\n",
    "    print(exception.strerror)\n",
    "else: \n",
    "    print(\"Directory is successfully created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Optimisation & Matrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining loss funciton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('losses_calc'):\n",
    "    loss= tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=TARGET, logits=output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('Optimizer'):\n",
    "    optimizer= tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    trian_step= optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Accuracy matrics & summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('Accuracy_calc'): \n",
    "    model_prediction= tf.argmax(output, axis=1, name='prediction')\n",
    "    correct_pred= tf.equal(model_prediction, tf.argmax(TARGET,axis=1)) # axis 1 is x axis of matrix\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('Perfomance'):\n",
    "    tf.summary.scalar('accuracy',accuracy)\n",
    "    tf.summary.scalar('cost',loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name input image is illegal; using input_image instead.\n"
     ]
    }
   ],
   "source": [
    "with tf.name_scope(\"show_image\"):\n",
    "    features_image= tf.reshape(FEATURES,[-1,28,28,1])\n",
    "    tf.summary.image('input image', features_image, max_outputs=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess= tf.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup Filewriter and Merge Summery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_summery = tf.summary.merge_all()\n",
    "\n",
    "train_writer= tf.summary.FileWriter(directory + '/train')\n",
    "train_writer.add_graph(sess.graph)\n",
    "\n",
    "validation_writer = tf.summary.FileWriter(directory+ '/validation')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize all the variables\n",
    "init= tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batching the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_of_batch = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total iterations:  50\n"
     ]
    }
   ],
   "source": [
    "num_examples= target_train.shape[0]\n",
    "nr_iterations= int(num_examples/size_of_batch)\n",
    "print(\"Total iterations: \",nr_iterations)\n",
    "index_in_epoch=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_batch(batch_size, data, labels):\n",
    "    \n",
    "    global num_examples \n",
    "    global index_in_epoch \n",
    "    \n",
    "    start=index_in_epoch\n",
    "    index_in_epoch += batch_size\n",
    "    \n",
    "    if index_in_epoch> num_examples:\n",
    "        start=0\n",
    "        index_in_epoch= batch_size\n",
    "    \n",
    "    \n",
    "    end= index_in_epoch\n",
    "    \n",
    "    return data[start:end], labels[start:end]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 \t| Training Accuracy = 0.8700000047683716\n",
      "Epoch 1 \t| Training Accuracy = 0.8799999952316284\n",
      "Epoch 2 \t| Training Accuracy = 0.8849999904632568\n",
      "Epoch 3 \t| Training Accuracy = 0.9580000042915344\n",
      "Epoch 4 \t| Training Accuracy = 0.9700000286102295\n",
      "Epoch 5 \t| Training Accuracy = 0.9789999723434448\n",
      "Epoch 6 \t| Training Accuracy = 0.9750000238418579\n",
      "Epoch 7 \t| Training Accuracy = 0.9810000061988831\n",
      "Epoch 8 \t| Training Accuracy = 0.9739999771118164\n",
      "Epoch 9 \t| Training Accuracy = 0.9810000061988831\n",
      "Epoch 10 \t| Training Accuracy = 0.9869999885559082\n",
      "Epoch 11 \t| Training Accuracy = 0.9829999804496765\n",
      "Epoch 12 \t| Training Accuracy = 0.9829999804496765\n",
      "Epoch 13 \t| Training Accuracy = 0.9860000014305115\n",
      "Epoch 14 \t| Training Accuracy = 0.9879999756813049\n",
      "Epoch 15 \t| Training Accuracy = 0.9900000095367432\n",
      "Epoch 16 \t| Training Accuracy = 0.9900000095367432\n",
      "Epoch 17 \t| Training Accuracy = 0.9890000224113464\n",
      "Epoch 18 \t| Training Accuracy = 0.9890000224113464\n",
      "Epoch 19 \t| Training Accuracy = 0.9890000224113464\n",
      "Epoch 20 \t| Training Accuracy = 0.9890000224113464\n",
      "Epoch 21 \t| Training Accuracy = 0.9919999837875366\n",
      "Epoch 22 \t| Training Accuracy = 0.9919999837875366\n",
      "Epoch 23 \t| Training Accuracy = 0.9900000095367432\n",
      "Epoch 24 \t| Training Accuracy = 0.9909999966621399\n",
      "Epoch 25 \t| Training Accuracy = 0.9900000095367432\n",
      "Epoch 26 \t| Training Accuracy = 0.9909999966621399\n",
      "Epoch 27 \t| Training Accuracy = 0.9909999966621399\n",
      "Epoch 28 \t| Training Accuracy = 0.9900000095367432\n",
      "Epoch 29 \t| Training Accuracy = 0.9919999837875366\n",
      "Epoch 30 \t| Training Accuracy = 0.9909999966621399\n",
      "Epoch 31 \t| Training Accuracy = 0.9919999837875366\n",
      "Epoch 32 \t| Training Accuracy = 0.9909999966621399\n",
      "Epoch 33 \t| Training Accuracy = 0.9919999837875366\n",
      "Epoch 34 \t| Training Accuracy = 0.9919999837875366\n",
      "Epoch 35 \t| Training Accuracy = 0.9919999837875366\n",
      "Epoch 36 \t| Training Accuracy = 0.9900000095367432\n",
      "Epoch 37 \t| Training Accuracy = 0.9900000095367432\n",
      "Epoch 38 \t| Training Accuracy = 0.9919999837875366\n",
      "Epoch 39 \t| Training Accuracy = 0.9919999837875366\n",
      "Epoch 40 \t| Training Accuracy = 0.9909999966621399\n",
      "Epoch 41 \t| Training Accuracy = 0.9919999837875366\n",
      "Epoch 42 \t| Training Accuracy = 0.9909999966621399\n",
      "Epoch 43 \t| Training Accuracy = 0.9919999837875366\n",
      "Epoch 44 \t| Training Accuracy = 0.9909999966621399\n",
      "Epoch 45 \t| Training Accuracy = 0.9919999837875366\n",
      "Epoch 46 \t| Training Accuracy = 0.9919999837875366\n",
      "Epoch 47 \t| Training Accuracy = 0.9909999966621399\n",
      "Epoch 48 \t| Training Accuracy = 0.9919999837875366\n",
      "Epoch 49 \t| Training Accuracy = 0.9919999837875366\n",
      "Done training!\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(nr_epoch):\n",
    "    \n",
    "    # ============= Training Dataset =========\n",
    "    for i in range(nr_iterations):\n",
    "        \n",
    "        batch_features, batch_target = next_batch(batch_size=size_of_batch, data=features_train, labels=target_train)\n",
    "        \n",
    "        feed_dictionary = {FEATURES:batch_features, TARGET:batch_target}\n",
    "        \n",
    "        sess.run(trian_step, feed_dict=feed_dictionary)\n",
    "        \n",
    "    \n",
    "    summary, batch_accuracy = sess.run(fetches=[merged_summery, accuracy], feed_dict=feed_dictionary)\n",
    "        \n",
    "    train_writer.add_summary(summary, epoch)\n",
    "    \n",
    "    print(f'Epoch {epoch} \\t| Training Accuracy = {batch_accuracy}')\n",
    "    \n",
    "    # ================== Validation ======================\n",
    "    \n",
    "    summary = sess.run(fetches=merged_summery, feed_dict={FEATURES:features_val, TARGET:target_val})\n",
    "    validation_writer.add_summary(summary, epoch)\n",
    "\n",
    "print('Done training!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-34-09b0257ad028>:3: simple_save (from tensorflow.python.saved_model.simple_save) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.simple_save.\n",
      "WARNING:tensorflow:From /opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/saved_model/signature_def_utils_impl.py:205: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: SavedModel/saved_model.pb\n"
     ]
    }
   ],
   "source": [
    "outputs= {'accuracy_calc/prediction':model_prediction}\n",
    "inputs= {'FEATURES': FEATURES}\n",
    "tf.compat.v1.saved_model.simple_save(sess, 'SavedModel', inputs, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make a Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "img= Image.open('MNIST/test_img.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "bw= img.convert('L')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_array= np.invert(bw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img = img_array.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = sess.run(fetches=tf.argmax(output, axis=1), feed_dict={FEATURES:[test_img ]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediciton for the test images is [2]\n"
     ]
    }
   ],
   "source": [
    "print(f'Prediciton for the test images is {prediction}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set is : 97.85%\n"
     ]
    }
   ],
   "source": [
    "test_accuracy= sess.run(fetches=accuracy, feed_dict={FEATURES:features_test, TARGET:target_test})\n",
    "print(f'Accuracy on test set is : {test_accuracy:0.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reset for the next run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_writer.close()\n",
    "validation_writer.close()\n",
    "sess.close()\n",
    "tf.reset_default_graph()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
